# -*- coding: utf-8 -*-
"""hypothAIze.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11L-eT9wx_kcA3rZIGt0l5u7rIWUHwl84
"""

import streamlit as st
import arxiv
import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer
from bert_score import score as bert_score
import torch

# Initialize model and tokenizer (downloaded only once)
@st.cache_resource
def load_model():
    model_name = "t5-small"
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    return model, tokenizer

model, tokenizer = load_model()

# Initialize sentence-transformers model for embeddings
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')  # Make sure to use a domain-specific or updated model

embedding_model = load_embedding_model()

# Define function to fetch papers from arXiv based on user input
def fetch_arxiv_papers(category, max_results=10):
    search_query = f"cat:{category}"
    search = arxiv.Search(
        query=search_query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    papers = []
    for paper in search.results():
        papers.append({"title": paper.title, "abstract": paper.summary})
    return pd.DataFrame(papers)

# Define function to generate hypothesis with adjustments for uniqueness
def generate_hypothesis(abstract_text):
    # Enhanced and more specific prompt to reduce vagueness
    input_text = f"Based on the following scientific abstract, generate a novel hypothesis about gene regulation that could further scientific understanding or applications in disease prevention: {abstract_text}"

    # Tokenize the input text and generate hypotheses
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    # Adding temperature and top-p sampling for more diverse outputs
    outputs = model.generate(
        **inputs,
        max_length=150,
        do_sample=True,           # Activates sampling instead of greedy decoding
        temperature=1.0,          # Increased temperature for more diversity
        top_p=0.95,               # Top-p (nucleus) sampling to reduce reliance on top predictions
        num_return_sequences=1    # Return one unique sequence
    )

    hypothesis = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Ensure hypothesis is unique by truncating any repeated start phrases
    if hypothesis.lower().startswith("generate a novel hypothesis"):
        hypothesis = hypothesis[len("Based on the following scientific abstract, generate a novel hypothesis about gene regulation that could further scientific understanding or applications in disease prevention: "):].strip()

    return hypothesis

# Function to calculate BERTScore
def calculate_bertscore(reference, hypothesis):
    P, R, F1 = bert_score([hypothesis], [reference], lang="en")
    return F1.mean().item()  # F1 score as accuracy metric

# Function to calculate Cosine Similarity with embeddings
def calculate_cosine_similarity(abstract, hypothesis):
    abstract_embedding = embedding_model.encode([abstract])[0]
    hypothesis_embedding = embedding_model.encode([hypothesis])[0]
    similarity = torch.nn.functional.cosine_similarity(
        torch.tensor(abstract_embedding), torch.tensor(hypothesis_embedding), dim=0
    )
    return similarity.item()

# Streamlit app title and description
st.title("AI-Driven Hypothesis Generator")
st.write("This tool generates hypotheses based on abstracts from arXiv. Enter a category to get started.")

# User input for category
category = st.text_input("Enter arXiv category (e.g., cs.AI for AI, physics.gen-ph for General Physics):", "cs.AI")
max_results = st.slider("Select the number of papers to retrieve:", 1, 100, 10)
use_references = st.checkbox("Use reference hypotheses for evaluation (BERTScore)")

# Input reference hypotheses if selected
reference_hypotheses = []
if use_references:
    st.write("Enter one reference hypothesis per abstract (in the same order).")
    for i in range(max_results):
        ref = st.text_input(f"Reference Hypothesis {i + 1}", "")
        reference_hypotheses.append(ref)

# Button to fetch and process data
if st.button("Generate Hypotheses", key="generate_hypotheses"):
    if not category:
        st.error("Please enter a valid arXiv category.")
    else:
        # Fetch papers
        st.info(f"Fetching papers from arXiv for category '{category}'...")
        df = fetch_arxiv_papers(category, max_results=max_results)

        # Generate hypotheses if papers were found
        if df.empty:
            st.warning("No papers found for the selected category.")
        else:
            st.success("Papers retrieved! Generating hypotheses...")

            # Apply hypothesis generation to each abstract
            df['generated_hypothesis'] = df['abstract'].apply(generate_hypothesis)

            # Evaluate hypotheses
            if use_references and len(reference_hypotheses) == len(df):
                # BERTScore calculation if references are available
                st.write("Calculating BERTScore for generated hypotheses...")
                df['bertscore'] = df.apply(
                    lambda row: calculate_bertscore(reference_hypotheses[row.name], row['generated_hypothesis']),
                    axis=1
                )
            else:
                # Cosine similarity calculation if references are not used
                st.write("Calculating Cosine Similarity for generated hypotheses...")
                df['similarity_score'] = df.apply(
                    lambda row: calculate_cosine_similarity(row['abstract'], row['generated_hypothesis']),
                    axis=1
                )

            # Display results
            for index, row in df.iterrows():
                st.subheader(f"Paper Title: {row['title']}")
                st.write(f"Abstract: {row['abstract']}")
                st.write(f"Generated Hypothesis: {row['generated_hypothesis']}")
                if use_references and 'bertscore' in df.columns:
                    st.write(f"BERTScore: {row['bertscore']:.4f}")
                elif 'similarity_score' in df.columns:
                    st.write(f"Cosine Similarity: {row['similarity_score']:.4f}")
                st.write("---")

            # Option to download as CSV
            csv = df.to_csv(index=False)
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name=f"arxiv_{category}_hypotheses.csv",
                mime="text/csv"
            )